{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Style Transfer Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leviv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ipdb\n",
    "import string\n",
    "import nltk\n",
    "import ipdb\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter accounts that we have data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw/AyannaPressley.json', 'raw/NatlParkService.json', 'raw/dril.json', 'raw/DalaiLama.json', 'raw/ddlovato.json', 'raw/AOC.json', 'raw/jimmyfallon.json', 'raw/TheEllenShow.json', 'raw/Mike_Pence.json', 'raw/BarackObama.json', 'raw/ladygaga.json', 'raw/elonmusk.json', 'raw/katyperry.json', 'raw/justinbieber.json', 'raw/SpeakerPelosi.json', 'raw/IlhanMN.json', 'raw/senatemajldr.json', 'raw/realDonaldTrump.json', 'raw/helper.json', 'raw/rihanna.json', 'raw/JohnCornyn.json', 'raw/UTAustin.json', 'raw/JoeBiden.json', 'raw/NPRHealth.json', 'raw/tedcruz.json', 'raw/austintexasgov.json']\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"raw\"\n",
    "accounts = []\n",
    "for f in listdir(raw_path): \n",
    "    if isfile(join(raw_path, f)):\n",
    "        accounts.append(raw_path + \"/\" + f)\n",
    "print(accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove links and reply tweets from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all Twitter metadata besides text\n",
    "def tweet_text_only (filename):\n",
    "    with open(filename) as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "    tweets = data['tweets']\n",
    "    cleaned_tweets = []\n",
    "    avg_num_tokens = 0\n",
    "    avg_tweet_len = 0\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        tweet = parse_tweet(tweet['full_text'])\n",
    "        if (len(tweet) > 0):\n",
    "            cleaned_tweets.append(tweet)\n",
    "            \n",
    "        avg_num_tokens += len(tweet.split())\n",
    "        avg_tweet_len += len(tweet)\n",
    "    \n",
    "    avg_num_tokens /= len(cleaned_tweets)\n",
    "    avg_tweet_len /= len(cleaned_tweets)\n",
    "    \n",
    "    return (cleaned_tweets, avg_num_tokens, avg_tweet_len)\n",
    "            \n",
    "\n",
    "# Parse a single tweet\n",
    "def parse_tweet (tweet):\n",
    "    # If the tweet is a reply, don't include it\n",
    "    if tweet[0] == '@':\n",
    "        return ''\n",
    "    \n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output each of the cleaned tweets as a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9cc2a6f90bdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mtrain_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_str_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Tweets\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.train.labels'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrain_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mtrain_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_str_rep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Account\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tweets.test.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtest_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-9cc2a6f90bdb>\u001b[0m in \u001b[0;36mget_str_rep\u001b[0;34m(tweets)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtweets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mcontent\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "def get_str_rep(tweets):\n",
    "    content = ''\n",
    "    for tweet in tweets:\n",
    "        content += (tweet + \"\\n\")\n",
    "        \n",
    "    return content[:-1]\n",
    "\n",
    "# Dataframe\n",
    "df = pd.DataFrame(columns = ['Tweets', 'Account'])\n",
    "sizes = []\n",
    "avg_tokens = []\n",
    "avg_lens = []\n",
    "\n",
    "for i, file in enumerate(accounts):\n",
    "    (tweets, avg_num_tokens, avg_tweet_len) = tweet_text_only(file)\n",
    "    \n",
    "    sizes.append(len(tweets))\n",
    "    avg_tokens.append(avg_num_tokens)\n",
    "    avg_lens.append(avg_tweet_len)\n",
    "    append = pd.DataFrame({\"Tweets\": tweets, \"Account\": i})\n",
    "        \n",
    "    df = df.append(append) # Append tweets and labels\n",
    "    \n",
    "# Only get the most recent tweets that balance the dataset\n",
    "min_samples = min(sizes)\n",
    "final_df = pd.DataFrame(columns = ['Tweets', 'Account'])\n",
    "i = 0\n",
    "\n",
    "for size in sizes:\n",
    "    final_df = final_df.append(df.iloc[i:(i+ min_samples)])\n",
    "    i += size\n",
    "    \n",
    "train, test = train_test_split(final_df, test_size=0.3, random_state=42)\n",
    "# Output a cleaned version of the data\n",
    "with open('tweets.train.txt', 'w') as train_file:\n",
    "    train_file.write((train[\"Tweets\"]))\n",
    "with open('tweets.train.labels', 'w') as train_label:\n",
    "    train_label.write(get_str_rep(train[\"Account\"]))\n",
    "\n",
    "with open('tweets.test.txt', 'w') as test_file:\n",
    "    test_file.write(get_str_rep(test[\"Tweets\"]))\n",
    "with open('tweets.test.labels', 'w') as test_label:\n",
    "    test_label.write(get_str_rep(test[\"Account\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate plots to describe the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.bar([\"@\" + file[4:-5] for file in accounts], sizes, color='#FEAF49')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Twitter accounts\")\n",
    "plt.ylabel(\"Number of tweets\")\n",
    "plt.title(\"Number of cleaned tweets per twitter account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.bar([\"@\" + file[4:-5] for file in accounts], avg_tokens, color='#45B8C2')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Twitter accounts\")\n",
    "plt.ylabel(\"Average number of tokens\")\n",
    "plt.title(\"Average number of tweet tokens per Twitter account\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.figure(figsize=(11, 8))\n",
    "plt.bar([\"@\" + file[4:-5] for file in accounts], avg_lens, color='#034C79')\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel(\"Twitter accounts\")\n",
    "plt.ylabel(\"Average tweet length\")\n",
    "plt.title(\"Average tweet length per Twitter account\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the cleaned data to create a vocab for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    \"\"\"\n",
    "    cleans tweets for a single account and return unique tokens.\n",
    "    \"\"\"\n",
    "    uniqueTokens = set()\n",
    "    \n",
    "    # clean tweets\n",
    "    for tweet in tweets:\n",
    "\n",
    "        tokens = tweet.lower().strip().split()\n",
    "\n",
    "        # remove punctuation and stopwords\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out non-alphabetic words and stopwords \n",
    "        sw = set(stopwords.words('english'))\n",
    "        tokens = list(filter(lambda x: x.isalpha() and x not in sw, tokens))\n",
    "\n",
    "        # filter out short tokens\n",
    "        tokens = list(filter(lambda x: len(x) > 1, tokens))\n",
    "\n",
    "        uniqueTokens.update(tokens)\n",
    "        \n",
    "    return uniqueTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary for all twitter accounts\n",
    "\n",
    "def addDocToVocab(account:str, vocab:Counter):\n",
    "    \"\"\"\n",
    "    Reads tweets for given twitter account,\n",
    "    cleans the tweets, and adds unique tokens\n",
    "    to the global vocabulary.\n",
    "    \"\"\"\n",
    "    tweets = tweet_text_only(account)[0]\n",
    "    tokens = clean_tweets(tweets)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def processAccounts(accounts:List):\n",
    "    \"\"\"\n",
    "    Adds tweets in each account to the global vocabulary\n",
    "    \n",
    "    :arg accounts:list of account names in the data directory\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for account in accounts:\n",
    "        tweetFile = f'{account}'\n",
    "        addDocToVocab(tweetFile, vocab)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processAccounts(accounts)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "# remove words with a frequency less than 2\n",
    "corpusVocab = [k for k,c in vocab.items() if c >= 2]\n",
    "\n",
    "print(len(corpusVocab))\n",
    "\n",
    "# save the vocab\n",
    "with open('vocab.txt', 'w') as vocabFile:\n",
    "    corpusVocab = '\\n'.join(corpusVocab)\n",
    "    vocabFile.write(corpusVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
