{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Style Transfer Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import ipdb\n",
    "import string\n",
    "import nltk\n",
    "import ipdb\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove links and reply tweets from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all Twitter metadata besides text\n",
    "def tweet_text_only (filename):\n",
    "    \n",
    "    with open(filename, encoding='utf-8') as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "    tweets = data['tweets']\n",
    "    cleaned_tweets = []\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        tweet = parse_tweet(tweet['full_text'])\n",
    "        if (len(tweet) > 0):\n",
    "            cleaned_tweets.append(tweet)\n",
    "#             print(tweet)\n",
    "            \n",
    "    return cleaned_tweets\n",
    "\n",
    "# Parse a single tweet\n",
    "def parse_tweet (tweet):\n",
    "    # If the tweet is a reply, don't include it\n",
    "    if tweet[0] == '@':\n",
    "        return ''\n",
    "    \n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output each of the cleaned tweets as a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['dril','dalai','elon','trump']\n",
    "\n",
    "for file in files:\n",
    "    \n",
    "    tweets = tweet_text_only(file + '.json')\n",
    "    \n",
    "    ipdb.set_trace()\n",
    "    \n",
    "    data_file = open(f'{file}_clean.csv', 'w', encoding='utf-8') \n",
    "  \n",
    "    # create the csv writer object \n",
    "    csv_writer = csv.writer(data_file) \n",
    "    \n",
    "    csv_writer.writerows(tweets)\n",
    "    \n",
    "#     # Output a cleaned version of the data\n",
    "#     with open(file + '_clean.txt', 'w') as json_file:\n",
    "#         json.dump(tweets, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweets(tweets):\n",
    "    \"\"\"\n",
    "    cleans tweets for a single account and return unique tokens.\n",
    "    \"\"\"\n",
    "    uniqueTokens = set()\n",
    "    \n",
    "    # clean tweets\n",
    "    for tweet in tweets:\n",
    "\n",
    "        tokens = tweet.lower().strip().split()\n",
    "\n",
    "        # remove punctuation and stopwords\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out non-alphabetic words and stopwords \n",
    "        sw = set(stopwords.words('english'))\n",
    "        tokens = list(filter(lambda x: x.isalpha() and x not in sw, tokens))\n",
    "\n",
    "        # filter out short tokens\n",
    "        tokens = list(filter(lambda x: len(x) > 1, tokens))\n",
    "\n",
    "        uniqueTokens.update(tokens)\n",
    "        \n",
    "    return uniqueTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary for all twitter accounts\n",
    "\n",
    "def addDocToVocab(account:str, vocab:Counter):\n",
    "    \"\"\"\n",
    "    Reads tweets for given twitter account,\n",
    "    cleans the tweets, and adds unique tokens\n",
    "    to the global vocabulary.\n",
    "    \"\"\"\n",
    "    tweets = tweet_text_only(account)\n",
    "    tokens = clean_tweets(tweets)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def processAccounts(accounts:List):\n",
    "    \"\"\"\n",
    "    Adds tweets in each account to the global vocabulary\n",
    "    \n",
    "    :arg accounts:list of account names in the data directory\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for account in accounts:\n",
    "        tweetFile = f'{account}.json'\n",
    "        addDocToVocab(tweetFile, vocab)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = processAccounts(['dril','dalai','elon','trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45295\n",
      "10044\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))\n",
    "\n",
    "# remove words with a frequency less than 2\n",
    "corpusVocab = [k for k,c in vocab.items() if c >= 2]\n",
    "\n",
    "print(len(corpusVocab))\n",
    "\n",
    "# save the vocab\n",
    "with open('vocab.txt', 'w') as vocabFile:\n",
    "    corpusVocab = '\\n'.join(corpusVocab)\n",
    "    vocabFile.write(corpusVocab)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
