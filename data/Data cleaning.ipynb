{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Style Transfer Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/leviv/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import ipdb\n",
    "import string\n",
    "import nltk\n",
    "import ipdb\n",
    "from typing import List\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter accounts that we have data for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['raw/dril.json', 'raw/DalaiLama.json', 'raw/elonmusk.json', 'raw/realDonaldTrump.json']\n"
     ]
    }
   ],
   "source": [
    "raw_path = \"raw\"\n",
    "accounts = []\n",
    "for f in listdir(raw_path): \n",
    "    if isfile(join(raw_path, f)):\n",
    "        accounts.append(raw_path + \"/\" + f)\n",
    "print(accounts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove links and reply tweets from the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore all Twitter metadata besides text\n",
    "def tweet_text_only (filename):\n",
    "    with open(filename) as f:\n",
    "      data = json.load(f)\n",
    "\n",
    "    tweets = data['tweets']\n",
    "    cleaned_tweets = []\n",
    "        \n",
    "    for tweet in tweets:\n",
    "        tweet = parse_tweet(tweet['full_text'])\n",
    "        if (len(tweet) > 0):\n",
    "            cleaned_tweets.append(tweet)\n",
    "            \n",
    "    return cleaned_tweets\n",
    "            \n",
    "\n",
    "# Parse a single tweet\n",
    "def parse_tweet (tweet):\n",
    "    # If the tweet is a reply, don't include it\n",
    "    if tweet[0] == '@':\n",
    "        return ''\n",
    "    \n",
    "    # Remove links\n",
    "    tweet = re.sub(r\"http\\S+\", \"\", tweet)\n",
    "    \n",
    "    return tweet.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output each of the cleaned tweets as a new file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_rep(tweets):\n",
    "    content = ''\n",
    "    for tweet in tweets:\n",
    "        content += (tweet + \"\\n\")\n",
    "        \n",
    "    return content[:-1]\n",
    "\n",
    "# Dataframe\n",
    "df = pd.DataFrame(columns = ['Tweets', 'Account'])\n",
    "sizes = []\n",
    "\n",
    "for file in accounts:\n",
    "    tweets = tweet_text_only(file)\n",
    "    sizes.append(len(tweets))\n",
    "    append = pd.DataFrame({\"Tweets\": tweets, \"Account\": file[4:]})\n",
    "    \n",
    "    df = df.append(append) # Append tweets and labels\n",
    "    \n",
    "# Only get the most recent tweets that balance the dataset\n",
    "min_samples = min(sizes)\n",
    "final_df = pd.DataFrame(columns = ['Tweets', 'Account'])\n",
    "i = 0\n",
    "\n",
    "for size in sizes:\n",
    "    final_df = final_df.append(df.iloc[i:(i+ min_samples)])\n",
    "    i += size\n",
    "    \n",
    "train, test = train_test_split(final_df, test_size=0.3, random_state=42)\n",
    "# Output a cleaned version of the data\n",
    "with open('tweets.train.txt', 'w') as train_file:\n",
    "    train_file.write(get_str_rep(train[\"Tweets\"]))\n",
    "with open('tweets.train.labels', 'w') as train_label:\n",
    "    train_label.write(get_str_rep(train[\"Account\"]))\n",
    "\n",
    "with open('tweets.test.txt', 'w') as test_file:\n",
    "    test_file.write(get_str_rep(test[\"Tweets\"]))\n",
    "with open('tweets.test.labels', 'w') as test_label:\n",
    "    test_label.write(get_str_rep(test[\"Account\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the cleaned data to create a vocab for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweets(tweets):\n",
    "    \"\"\"\n",
    "    cleans tweets for a single account and return unique tokens.\n",
    "    \"\"\"\n",
    "    uniqueTokens = set()\n",
    "    \n",
    "    # clean tweets\n",
    "    for tweet in tweets:\n",
    "\n",
    "        tokens = tweet.lower().strip().split()\n",
    "\n",
    "        # remove punctuation and stopwords\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "        # filter out non-alphabetic words and stopwords \n",
    "        sw = set(stopwords.words('english'))\n",
    "        tokens = list(filter(lambda x: x.isalpha() and x not in sw, tokens))\n",
    "\n",
    "        # filter out short tokens\n",
    "        tokens = list(filter(lambda x: len(x) > 1, tokens))\n",
    "\n",
    "        uniqueTokens.update(tokens)\n",
    "        \n",
    "    return uniqueTokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary for all twitter accounts\n",
    "\n",
    "def addDocToVocab(account:str, vocab:Counter):\n",
    "    \"\"\"\n",
    "    Reads tweets for given twitter account,\n",
    "    cleans the tweets, and adds unique tokens\n",
    "    to the global vocabulary.\n",
    "    \"\"\"\n",
    "    tweets = tweet_text_only(account)\n",
    "    tokens = clean_tweets(tweets)\n",
    "    vocab.update(tokens)\n",
    "    \n",
    "def processAccounts(accounts:List):\n",
    "    \"\"\"\n",
    "    Adds tweets in each account to the global vocabulary\n",
    "    \n",
    "    :arg accounts:list of account names in the data directory\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "    \n",
    "    for account in accounts:\n",
    "        tweetFile = f'{account}'\n",
    "        addDocToVocab(tweetFile, vocab)\n",
    "        \n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45295\n",
      "10044\n"
     ]
    }
   ],
   "source": [
    "vocab = processAccounts(accounts)\n",
    "\n",
    "print(len(vocab))\n",
    "\n",
    "# remove words with a frequency less than 2\n",
    "corpusVocab = [k for k,c in vocab.items() if c >= 2]\n",
    "\n",
    "print(len(corpusVocab))\n",
    "\n",
    "# save the vocab\n",
    "with open('vocab.txt', 'w') as vocabFile:\n",
    "    corpusVocab = '\\n'.join(corpusVocab)\n",
    "    vocabFile.write(corpusVocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
