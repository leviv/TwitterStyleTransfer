{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/isaacbuitrago/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from torchtext import data\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# max chars in tweet\n",
    "TWEET_LEN = 280\n",
    "trainPath = './data/tweets.train.txt'\n",
    "trainLabelsPath = './data/tweets.train.labels'\n",
    "\n",
    "# load vocab\n",
    "with open('./data/vocab.txt', 'r', encoding='UTF-8') as file:\n",
    "    vocab = file.read().split()\n",
    "    vocab = set(vocab)\n",
    "\n",
    "# stopwords set\n",
    "sw = set(stopwords.words('english'))\n",
    "\n",
    "def cleanTweet(tweet):\n",
    "    # split into sentences\n",
    "    tokens = tweet.lower().split()\n",
    "\n",
    "    # filter out non-alphabetic, stopwords, short tokens, and tokens not in vocab\n",
    "    tokens = list(filter(lambda x: x.isalpha() and x not in sw \\\n",
    "                                   and len(x) > 1 and x in vocab, tokens))\n",
    "\n",
    "    # create new sentences\n",
    "    tokens = ' '.join(tokens)\n",
    "    return tokens\n",
    "\n",
    "def loadData(dataFile, labelsFile):\n",
    "    \"\"\"\n",
    "    Loads twitter data and it's related labels\n",
    "    \"\"\"\n",
    "    # first element is twitter data, second is labels\n",
    "    data = []\n",
    "    for path in [dataFile, labelsFile]:\n",
    "        with open(path, 'r', encoding='UTF-8') as file:\n",
    "            text = file.read().split('\\n')\n",
    "            data.append(text)\n",
    "            \n",
    "    data = np.array(data)\n",
    "    return data\n",
    "\n",
    "X_train, y_train = loadData(trainPath, trainLabelsPath)\n",
    "\n",
    "# clean training data\n",
    "X_train = np.array(list(map(cleanTweet, X_train)))\n",
    "\n",
    "# map words to integers\n",
    "# tokenizer = pre.text.Tokenizer()\n",
    "# tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# encode data\n",
    "# encodedTweets = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# X_train = pre.sequence.pad_sequences(encodedTweets, maxlen=TWEET_LEN, padding='post')\n",
    "\n",
    "# add dimension for sequence length, 1\n",
    "# X_train = np.expand_dims(X_train, 1).astype(np.float32)\n",
    "\n",
    "# create embedding layer\n",
    "# embedding = Embedding(input_dim=len(vocab), output_dim=100, input_length=TWEET_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(init_token='<start>', eos_token='<eos>', tokenize='spacy', fix_length=280)\n",
    "LABEL = data.Field(sequential=False, unk_token=None)\n",
    "    \n",
    "TEXT.build_vocab(list(vocab))\n",
    "LABEL.build_vocab(y_train)\n",
    "\n",
    "n_vocab = len(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<start>',\n",
       " '<eos>',\n",
       " 'e',\n",
       " 'i',\n",
       " 's',\n",
       " 'a',\n",
       " 'n',\n",
       " 'r',\n",
       " 't',\n",
       " 'o',\n",
       " 'l',\n",
       " 'c',\n",
       " 'd',\n",
       " 'u',\n",
       " 'g',\n",
       " 'p',\n",
       " 'm',\n",
       " 'h',\n",
       " 'y',\n",
       " 'b',\n",
       " 'f',\n",
       " 'v',\n",
       " 'w',\n",
       " 'k',\n",
       " 'x',\n",
       " 'j',\n",
       " 'z',\n",
       " 'q']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# text = TEXT.process([\"I love candy\", \"I'm the fucking greatest\"])\n",
    "\n",
    "TEXT.vocab.itos"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}